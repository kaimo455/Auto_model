{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     12,
     14,
     46,
     53,
     65,
     91,
     122
    ]
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import lightgbm\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt.observer import JSONLogger\n",
    "from bayes_opt.event import Events\n",
    "from bayes_opt.util import load_logs\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "\n",
    "class LightGBM_binary_bayes_opt:\n",
    "\n",
    "    def __init__(self, X_train, y_train, X_eval, y_eval, X_test, y_test, \\\n",
    "                 base_params:dict, cat_params:dict, \\\n",
    "                 num_params:dict, int_params:dict, \\\n",
    "                 load_log=False, num_opts=100, score_ouput='./score.json'):\n",
    "        \"\"\"\n",
    "        Class instance initializer.\n",
    "        \"\"\"\n",
    "        # data\n",
    "        self.X_train, self.X_eval, self.X_test = X_train, X_eval, X_test\n",
    "        self.y_train, self.y_eval, self.y_test = y_train, y_eval, y_test\n",
    "        # hyperparamters\n",
    "        self.base_params = base_params\n",
    "        self.cat_params = cat_params\n",
    "        self.num_params = num_params\n",
    "        self.int_params = int_params\n",
    "        self.cat_params_combinations = []\n",
    "        self.optimizers = []\n",
    "        self.current_params = None\n",
    "        self.all_params = None\n",
    "        # bayesian optimizer\n",
    "        self.load_log = load_log\n",
    "        self.num_opts = num_opts\n",
    "        # initializing\n",
    "        self._generate_cat_params_combination()\n",
    "        # models\n",
    "        self.lgbs = []\n",
    "        # metrics\n",
    "        self.score_ouput = score_ouput\n",
    "        self.train_score = []\n",
    "        self.eval_score = []\n",
    "        self.test_score = []\n",
    "\n",
    "    def _generate_cat_params_combination(self):\n",
    "        \"\"\"\n",
    "        Generate categorical hyperparameters combinations.\n",
    "        \"\"\"\n",
    "        for combination in itertools.product(*self.cat_params.values()):\n",
    "            self.cat_params_combinations.append(dict(zip(cat_params.keys(), combination)))\n",
    "\n",
    "    def _object_score(self, **params_to_optimize):\n",
    "        \"\"\"\n",
    "        Call by Bayeisan optimizer to maximize.\n",
    "        \"\"\"\n",
    "        # convert int dtype parameters in 'num_params' to int parameters\n",
    "        for _int_param in self.int_params:\n",
    "            if _int_param in params_to_optimize:\n",
    "                params_to_optimize[_int_param] = int(params_to_optimize[_int_param])\n",
    "        # combine all hyperparameters\n",
    "        self.all_params = dict(self.current_params, **params_to_optimize)\n",
    "        return self.train_lgb(self.all_params)\n",
    "\n",
    "    def train_lgb(self, all_params:dict):\n",
    "        \"\"\"\n",
    "        Using all hyperparameters to train LightGBM. Return the object function score.\n",
    "        \"\"\"\n",
    "        train_dataset = lightgbm.Dataset(data=self.X_train, label=self.y_train)\n",
    "        eval_dataset = lightgbm.Dataset(data=self.X_eval, label=self.y_eval)\n",
    "        test_dataset = lightgbm.Dataset(data=self.X_test, label=self.y_test)\n",
    "        lgb_clf = lightgbm.train(params=all_params, \\\n",
    "                                 train_set=train_dataset, \\\n",
    "                                 num_boost_round=3000, \\\n",
    "                                 valid_sets=[train_dataset, eval_dataset], \\\n",
    "                                 valid_names=['Train', 'Eval'], \\\n",
    "                                 early_stopping_rounds = 400, \\\n",
    "                                 verbose_eval = -1, \\\n",
    "                                 learning_rates=lambda iters: 0.6 * (0.99 ** iters))\n",
    "        # store model\n",
    "        self.lgbs.append(lgb_clf)\n",
    "        # store score and save to file\n",
    "        self.train_score.append(lgb_clf.best_score['Train'])\n",
    "        self.eval_score.append(lgb_clf.best_score['Eval'])\n",
    "        with open(self.score_ouput, 'a+') as f:\n",
    "            json.dump(lgb_clf.best_score, f)\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "        return lgb_clf.best_score['Eval']['auc']\n",
    "    \n",
    "    def optimize_lgb(self):\n",
    "        \"\"\"\n",
    "        The main entrence of optimizing LightGBM\n",
    "        \"\"\"\n",
    "        # we need to manuall go through the categorical hyperparameter combinations\n",
    "        # then apply Bayesian optimization\n",
    "        for idx, cat_params_combination in enumerate(self.cat_params_combinations):\n",
    "            print('Current categorical hyperparameters combination #{:d}: {}'\\\n",
    "                  .format(idx, cat_params_combination))\n",
    "            # combine base and current categorical combination to form current_params\n",
    "            self.current_params = dict(self.base_params, **cat_params_combination)\n",
    "            # create optimizer\n",
    "            optimizer = BayesianOptimization(f=self._object_score, \\\n",
    "                                             pbounds=self.num_params, \\\n",
    "                                             random_state=1213)\n",
    "            \n",
    "            # if previous log exist, load it and continue optimizing\n",
    "            log_path = \"./logs_{:d}.json\".format(idx)\n",
    "            if self.load_log:\n",
    "                load_logs(optimizer, logs=[log_path])\n",
    "                self.logger = JSONLogger(path=log_path)\n",
    "                optimizer.subscribe(Events.OPTMIZATION_STEP, self.logger)\n",
    "                optimizer.maximize(init_points=0, n_iter=self.num_opts)\n",
    "            else:\n",
    "                self.logger = JSONLogger(path=log_path)\n",
    "                optimizer.subscribe(Events.OPTMIZATION_STEP, self.logger)\n",
    "                optimizer.maximize(init_points=10, n_iter=self.num_opts)\n",
    "            \n",
    "            # save current opotimizer\n",
    "            self.optimizers.append(optimizer)\n",
    "\n",
    "class LightGBM_binary_hyperopt:\n",
    "    \"\"\"\n",
    "    Recommand to use hyperopt\n",
    "    \"\"\"\n",
    "    def __init__(self, X_train, y_train, X_eval, y_eval, X_test, y_test, \\\n",
    "                 base_params: dict, cat_params: dict, int_params: dict, float_params: dict, \\\n",
    "                 num_opts=1000, trials_path='./trials.pkl', load_trials=False, \\\n",
    "                 lgb_num_boost_round=3000, lgb_early_stopping_rounds=400):\n",
    "        \"\"\"\n",
    "        Class instance initializer.\n",
    "        \"\"\"\n",
    "        # data\n",
    "        self.X_train, self.X_eval, self.X_test = X_train, X_eval, X_test\n",
    "        self.y_train, self.y_eval, self.y_test = y_train, y_eval, y_test\n",
    "        self.train_dataset = lightgbm.Dataset(data=self.X_train, label=self.y_train)\n",
    "        self.eval_dataset = lightgbm.Dataset(data=self.X_eval, label=self.y_eval)\n",
    "        self.test_dateset = lightgbm.Dataset(data=self.X_test, label=self.y_test)\n",
    "        # hyperparameters\n",
    "        self.base_params, self.cat_params, self.int_params, self.float_params = base_params, cat_params, int_params, float_params\n",
    "        self.all_params = self._init_params()\n",
    "        # lightgbm other hyperparameter\n",
    "        self.lgb_num_boost_round, self.lgb_early_stopping_rounds = lgb_num_boost_round, lgb_early_stopping_rounds\n",
    "        # optimizer\n",
    "        self.num_opts, self.trials_path, self.load_trials = num_opts, trials_path, load_trials\n",
    "        self.trials = self._init_trials()\n",
    "    \n",
    "    def _init_params(self):\n",
    "        \"\"\"\n",
    "        Initialize hyperparameters\n",
    "        \"\"\"\n",
    "        # categorical hyperparameters\n",
    "        self.cat_params_hp = {param: hp.choice(param, candidates) \\\n",
    "                      for param, candidates in self.cat_params.items()}\n",
    "        # integer hyperparameters\n",
    "        self.int_params_hp = {param: hp.choice(param, np.arange(*start_end_step, dtype=np.int)) \\\n",
    "                              for param, start_end_step in self.int_params.items()}\n",
    "        # float hyperparameters\n",
    "        self.float_params_hp = {param: hp.uniform(param, *candidates) \\\n",
    "                                for param, candidates in self.float_params.items()}\n",
    "        # generate all hyperparameters\n",
    "        return dict(self.base_params, \\\n",
    "                    **self.cat_params_hp, \\\n",
    "                    **self.int_params_hp, \\\n",
    "                    **self.float_params_hp)\n",
    "\n",
    "    def _init_trials(self):\n",
    "        \"\"\"\n",
    "        Initialize trials database\n",
    "        \"\"\"\n",
    "        if self.load_trials:\n",
    "            trials = pickle.load(open(self.trials_path, \"rb\"))\n",
    "            current_iter = len(trials.losses())\n",
    "            self.num_opts += current_iter\n",
    "        else:\n",
    "            trials = Trials()\n",
    "        return trials\n",
    "    \n",
    "    def _object_score(self, params):\n",
    "        \"\"\"\n",
    "        Using all hyperparameters to train LightGBM. Return the objective function score.\n",
    "        \"\"\"\n",
    "        lgb_clf = lightgbm.train(params=params, \\\n",
    "                                 train_set=self.train_dataset, \\\n",
    "                                 num_boost_round=self.lgb_num_boost_round, \\\n",
    "                                 valid_sets=[self.train_dataset, self.eval_dataset], \\\n",
    "                                 valid_names=['Train', 'Eval'], \\\n",
    "                                 early_stopping_rounds=self.lgb_early_stopping_rounds, \\\n",
    "                                 verbose_eval = -1, \\\n",
    "                                 learning_rates=lambda iters: 0.6 * (0.99 ** iters))\n",
    "        # we invoke difference between train auc and eval auc as penalty\n",
    "        # eval_auc - (train_auc - eval_auc)\n",
    "        # that is maximize inverse of the above formula\n",
    "        return {'loss': -(2*lgb_clf.best_score['Eval']['auc'] - lgb_clf.best_score['Train']['auc']), \\\n",
    "                'train_auc': lgb_clf.best_score['Train']['auc'], \\\n",
    "                'eval_auc': lgb_clf.best_score['Eval']['auc'], \\\n",
    "                'train_error': lgb_clf.best_score['Train']['binary_error'], \\\n",
    "                'eval_error': lgb_clf.best_score['Eval']['binary_error'], \\\n",
    "                'status': STATUS_OK}\n",
    "\n",
    "    def optimize_lgb(self):\n",
    "        \"\"\"\n",
    "        The main entrence of optimizing LightGBM\n",
    "        \"\"\"\n",
    "        best_params = fmin(self._object_score, self.all_params, algo=tpe.suggest, \\\n",
    "                           max_evals = self.num_opts, trials=self.trials)\n",
    "        # save trials for further fine-tune\n",
    "        pickle.dump(self.trials, open(self.trials_path, \"wb\"))\n",
    "        # store best hyperparameters\n",
    "        self.best_params = best_params\n",
    "        \n",
    "        return best_params\n",
    "\n",
    "    def best_model(self):\n",
    "        \"\"\"\n",
    "        Use best hyperparameters to train lihgtgbm model\n",
    "        \"\"\"\n",
    "        lgb_clf = lightgbm.train(params=self.best_params, \\\n",
    "                                 train_set=train_dataset, \\\n",
    "                                 num_boost_round=self.lgb_num_boost_round, \\\n",
    "                                 valid_sets=[train_dataset, eval_dataset], \\\n",
    "                                 valid_names=['Train', 'Eval'], \\\n",
    "                                 early_stopping_rounds=self.lgb_early_stopping_rounds, \\\n",
    "                                 verbose_eval = -1, \\\n",
    "                                 learning_rates=lambda iters: 0.6 * (0.99 ** iters))\n",
    "        return lgb_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# @ unused hyperparameters\n",
    "#               'drop_rate' | used only in dart\n",
    "#               'max_drop' | used only in dart\n",
    "#               'skip_drop' | used only in dart\n",
    "#               'xgboost_dart_mode' | used only in dart\n",
    "#               'uniform_drop' | used only in dart\n",
    "#               'drop_seed' | used only in dart\n",
    "#               'top_rate' | | for safety not used\n",
    "#               'other_rate' | for safety not used\n",
    "#               'top_k' | used only in Voteing parallel\n",
    "#               'monotone_constraints' | default\n",
    "#               'feature_contri' | default\n",
    "#               'forcedsplits_filename' | default\n",
    "#               'forcedbins_filename' | default\n",
    "#               'refit_decay_rate' | used only in refit task\n",
    "#               'cegb_penalty_feature_lazy' | default\n",
    "#               'cegb_penalty_feature_coupled' | default\n",
    "#               'cegb_tradeoff': (1, 10), \\\n",
    "#               'cegb_penalty_split': (0 ,10), \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "base_params = {'task': 'train', \\\n",
    "               'objective': 'binary', \\\n",
    "               'tree_learner': 'serial', \\\n",
    "               'num_threads': 4, \\\n",
    "               'device_type': 'cpu', \\\n",
    "               'seed': 1213, \\\n",
    "               'num_leaves': 500, \\\n",
    "               ## learning control parameters\n",
    "               'bagging_seed': 42, \\\n",
    "               'feature_fraction_seed': 3, \\\n",
    "               'first_metric_only': False, \\\n",
    "               'max_delta_step': 0, \\\n",
    "               'min_sum_hessian_in_leaf': 0.05, \\\n",
    "               'bagging_fraction': 1, \\\n",
    "               'pos_bagging_fraction': 1, \\\n",
    "               'neg_bagging_fraction': 1, \\\n",
    "               'bagging_freq': 0, \\\n",
    "               'feature_fraction': 1, \\\n",
    "               'feature_fraction_bynode': 1, \\\n",
    "               'min_data_in_leaf': 250, \\\n",
    "               'lambda_l1': 250, \\\n",
    "               ## IO parameters\n",
    "               'bin_construct_sample_cnt': 200000, \\\n",
    "               'histogram_pool_size': -1, \\\n",
    "               ## objective parameters\n",
    "               'is_unbalance': True, \\\n",
    "               'metric': 'auc,binary_logloss,binary_error', \\\n",
    "               'metric_freq': 1, \\\n",
    "               'max_bin': 511, \\\n",
    "               'min_data_in_bin': 3, \\\n",
    "               'min_gain_to_split': 0, \\\n",
    "               ### parameters for categorical features\n",
    "               'min_data_per_group': 100, \\\n",
    "               'max_cat_threshold': 32, \\\n",
    "               'cat_l2': 500, \\\n",
    "               'cat_smoth': 500\n",
    "              }\n",
    "\n",
    "cat_params = {'boosting': ['gbdt']}\n",
    "\n",
    "num_params = {\n",
    "#               'num_leaves': (2, 1024), \\\n",
    "              ## learning control parameters\n",
    "              'max_depth': (1, 200), \\\n",
    "#               'min_data_in_leaf': (2, 500), \\\n",
    "#               'min_sum_hessian_in_leaf': (0, 0.1), \\\n",
    "#               'bagging_fraction': (0.1, 1), \\\n",
    "#               'pos_bagging_fraction': (0.1, 1), \\\n",
    "#               'neg_bagging_fraction': (0.1, 1), \\\n",
    "#               'bagging_freq': (0, 100), \\\n",
    "#               'feature_fraction': (0.1, 1), \\\n",
    "#               'feature_fraction_bynode': (0.1, 1), \\\n",
    "#               'lambda_l1': (0, 500), \\\n",
    "              'lambda_l2': (1000, 4000), \\\n",
    "              ## objective parameters\n",
    "              'sigmoid': (0.1, 500), \\\n",
    "              ### parameters for categorical features\n",
    "#               'cat_l2': (10, 1000), \\\n",
    "#               'cat_smoth': (10, 1000), \\\n",
    "              'max_cat_to_onehot': (1, 100)\n",
    "             }\n",
    "\n",
    "int_params = ['num_leaves', 'max_depth', 'min_data_in_leaf', 'bagging_freq', \\\n",
    "              'min_data_per_group', 'max_cat_threshold', 'max_cat_to_onehot', \\\n",
    "              'max_bin', 'min_data_in_bin', 'max_cat_to_onehot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_bayes = LightGBM_binary_bayes_opt(X_train, y_train, X_eval, y_eval, X_test, y_test, \\\n",
    "                                      base_params, cat_params, num_params, int_params, \\\n",
    "                                      load_log=False, num_opts=500)\n",
    "lgb_bayes.optimize_lgb()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperopt optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_params = {'task': 'train', \\\n",
    "               'objective': 'binary', \\\n",
    "               'tree_learner': 'serial', \\\n",
    "               'num_threads': 4, \\\n",
    "               'device_type': 'cpu', \\\n",
    "               'seed': 1213, \\\n",
    "               'bagging_seed': 42, \\\n",
    "               'feature_fraction_seed': 3, \\\n",
    "               'first_metric_only': False, \\\n",
    "               'max_delta_step': 0, \\\n",
    "               'bin_construct_sample_cnt': 200000, \\\n",
    "               'histogram_pool_size': -1, \\\n",
    "               'is_unbalance': True, \\\n",
    "               'metric': 'auc,binary_logloss,binary_error', \\\n",
    "               'metric_freq': 1}\n",
    "\n",
    "cat_params = {'boosting': ['gbdt']}\n",
    "\n",
    "int_params = {'num_leaves': (2, 1024, 8), \\\n",
    "              'max_depth': (1, 100, 1), \\\n",
    "              'min_data_in_leaf': (2, 500, 4), \\\n",
    "              'bagging_freq': (0, 100, 1), \\\n",
    "              'min_data_per_group': (100, 500, 10), \\\n",
    "              'max_cat_threshold': (16, 256, 2), \\\n",
    "              'max_cat_to_onehot': (1, 100, 1), \\\n",
    "              'max_bin': (127, 511, 2), \\\n",
    "              'min_data_in_bin': (3, 128, 8)}\n",
    "\n",
    "float_params = {'min_sum_hessian_in_leaf': (0, 0.1), \\\n",
    "                'bagging_fraction': (0.1, 1), \\\n",
    "                'pos_bagging_fraction': (0.1, 1), \\\n",
    "                'neg_bagging_fraction': (0.1, 1), \\\n",
    "                'feature_fraction': (0.1, 1), \\\n",
    "                'feature_fraction_bynode': (0.1, 1), \\\n",
    "                'lambda_l1': (0, 500), \\\n",
    "                'lambda_l2': (1000, 4000), \\\n",
    "                'sigmoid': (0.1, 500), \\\n",
    "                'cat_l2': (10, 1000), \\\n",
    "                'cat_smoth': (10, 1000), \\\n",
    "                'min_gain_to_split': (0, 100)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_hyperopt = lightgbm_hyperopt_binary(X_train, y_train, X_eval, y_eval, X_test, y_test, \\\n",
    "                                        base_params, cat_params, int_params, float_params, \\\n",
    "                                        num_opts=50, trials_path='./trials.pkl', load_trials=True)\n",
    "best_params = lgb_hyperopt.optimize_lgb()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
